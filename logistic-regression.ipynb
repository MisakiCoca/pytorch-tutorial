{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "节选自 Kaggle：MNIST 可被视为计算机视觉领域上的 \"Hello World\" 数据集。自 1999 年发布以来，这个经典的手写图像数据集一直是分类算法的基准，随着深度学习技术的发展，MNIST 依旧是研究人员和学习者的一个可靠资源。下图展示了该数据集的部分数据：\n",
    "![MNIST-sample](assets/MnistExamples.png)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "该教程展示了：如何使用 PyTorch 进行简单的手写分类。部分代码参考了 PyTorch 官方教程内容 (BSD 3-Clause \"New\" or \"Revised\" License)。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在正式开始之前，我们对 PyTorch 进行安装。\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install torch torchvision"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "首先，我们需要对 MNIST 数据集进行下载并创建数据加载器，但幸运的是 PyTorch 官方已经集成了下载方式。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST  # 导入 MNIST 数据集类\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize  # 图像预处理\n",
    "\n",
    "# 创建用于训练的 MNIST 数据集 和 加载器 (DataLoader)\n",
    "train_mnist = MNIST(\n",
    "    root='./', train=True, download=True,  # 若此前指定路径不存在 MNIST 数据，则自动下载\n",
    "    transform=Compose([ToTensor(), Normalize((0.1307,), (0.3081,))]),  # 将 int[0, 255] 图像处理为 float32[0, 1] 并标准化\n",
    ")\n",
    "train_loader = DataLoader(train_mnist, batch_size=1024, shuffle=True)  # 设定一批数据由 1024 个图像构成，并打乱顺序抽取\n",
    "\n",
    "# 创建用于测试的 MNIST 数据集 和 加载器 (DataLoader)\n",
    "eval_mnist = MNIST(\n",
    "    root='./', train=False, download=True,  # 与 Train 不同的是，这里 train 属性为 False\n",
    "    transform=Compose([ToTensor(), Normalize((0.1307,), (0.3081,))]),\n",
    ")\n",
    "eval_loader = DataLoader(eval_mnist, batch_size=1024, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来，我们将定义一个简单的网络用于对图像进行回归分类。对于不用的应用，网络应当进行对应的设计，但是对于 MNIST 数据集来说，如此简单的网络便足够。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# nn 模块包含了 PyTorch 大部分的计算函数操作, Tensor 是 PyTorch 的基本计算单位-张量\n",
    "from torch import nn, Tensor, max_pool2d, relu, dropout, log_softmax\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    # 初始化函数\n",
    "    def __init__(self):\n",
    "        super().__init__()  # 进行超类的继承\n",
    "\n",
    "        # 图像特征提取\n",
    "        self.conv_1 = nn.Conv2d(1, 16, kernel_size=5)  # 从图像中提取 16 维的特征\n",
    "        self.conv_2 = nn.Conv2d(16, 32, kernel_size=5)  # 从特征中进一步提取 32 维的特征 这也是卷积神经网络的显著特征（连续多次卷积以提取更高维度特征）\n",
    "        self.drop = nn.Dropout2d()  # 随机丢失部分前向传播计算 用于防止网络对数据过拟合\n",
    "\n",
    "        # 利用高维度特征进行分类\n",
    "        self.fc_1 = nn.Linear(512, 64)  # 输入维度计算 (in_c) = 特征维度 (32) * 特征图像大小 (16)\n",
    "        self.fc_2 = nn.Linear(64, 10)  # 最后输出一个 10 维的数组，表示某张图片是 [0-9] 的概率\n",
    "\n",
    "    # 前向传播所使用的函数\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # 第一次特征提取\n",
    "        # [dim: 1, size: 28] -> [dim: 16, size: 24]\n",
    "        x = self.conv_1(x)\n",
    "        # [dim:16, size: 24] -> [dim: 16, size: 12]\n",
    "        x = max_pool2d(x, kernel_size=2)\n",
    "        # 激活函数 这里选择 ReLU (负值置零)\n",
    "        x = relu(x)\n",
    "\n",
    "        # 第二次特征提取\n",
    "        # [dim: 16, size: 12] -> [dim: 32, size: 8]\n",
    "        x = self.conv_2(x)\n",
    "        # 随机丢弃 防止过拟合\n",
    "        x = self.drop(x)\n",
    "        # [dim: 32, size: 8] -> [dim: 32, size: 4]\n",
    "        x = max_pool2d(x, kernel_size=2)\n",
    "        # 激活函数 这里选择 ReLU (负值置零)\n",
    "        x = relu(x)\n",
    "\n",
    "        # 将特征展开并使用线性分类进一步提取特征。注：也有使用卷积直至形成 1x1 的例子，称为 Fully-Convolution 网络，这里不使用。\n",
    "        # [dim: 32, size: 4] -> [dim: 512, size: 1]\n",
    "        x = x.view(-1, 512)\n",
    "        # [dim: 800, size: 1] -> [dim: 64, size: 1]\n",
    "        x = self.fc_1(x)\n",
    "        # 激活函数 这里选择 ReLU (负值置零)\n",
    "        x = relu(x)\n",
    "        # 随机丢弃 防止过拟合\n",
    "        x = dropout(x, p=0.05, train=self.training)  # 丢弃的函数式写法，此处意为：当训练时，有5%的概率丢弃学习的数据\n",
    "\n",
    "        # 根据特征进行最后的推断\n",
    "        # [dim: 64, size: 1] -> [dim: 10, size: 1]\n",
    "        x = self.fc_2(x)\n",
    "        # 激活函数 这里选择 LogSoftmax (使和为 1 后进行 log 操作)\n",
    "        # 相较于 Softmax，使用 LogSoftmax 可以更高地惩罚似然空间中更大的错误\n",
    "        x = log_softmax(x, dim=1)\n",
    "\n",
    "        # 返回各 [0-9] 网络预测的概率\n",
    "        # 当使用 Softmax 激活时，若值为 [0.1, 0.2, 0.04, ...] 意味着网络认为：这个图像为 0 的概率为 0.1，为 1 的概率为 0.2，以此类推\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "这样，我们便有了一个可以将图像进行分类的 \"人工智能\" \"深度学习\" 网络。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来，我们将构建这个网络的训练流程，将按照：「输入 -> 网络 -> 输出 -> 计算损失函数 -> 梯度反向传播」 这个过程进行。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import SGD\n",
    "from torch.nn.functional import nll_loss\n",
    "import torch\n",
    "\n",
    "# 创建一个 Net 的具体变量\n",
    "net = Net()\n",
    "# 若 CUDA 计算单元可用，将模型转移到 CUDA 中\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "\n",
    "# 创建一个优化器，这里使用 SGD，是最为朴素的梯度下降策略\n",
    "optimizer = SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "# 定义训练流程，epoch 表示当前是第 i 轮训练\n",
    "def train_process(epoch: int):\n",
    "    # 使网络进入训练模式，这将影响 BatchNorm, DropOut 等操作\n",
    "    net.train()\n",
    "\n",
    "    # 加载数据\n",
    "    tqdm_process = tqdm(enumerate(train_loader))  # 进度条\n",
    "    for batch_idx, (data, target) in tqdm_process:\n",
    "        # 若 CUDA 计算单元可用，将数据转移到 CUDA 中\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()  # 此处使用 .cuda()，对于高级用户 PyTorch 提供了更具体的操作 .to()\n",
    "\n",
    "        # 在一些古早的教程中，可能含有如下两行，是 PyTorch 0.4 的变量问题，目前已不再需要\n",
    "        # data, target = Variable(data), Variable(target)\n",
    "\n",
    "        # 前向传播 [size: 1024(batch_size), 1, 32, 32] -> net -> [size: 1024(batch_size), 10]\n",
    "        pred = net(data)\n",
    "\n",
    "        # 计算损失函数，这里选择 nll 损失\n",
    "        loss = nll_loss(pred, target)\n",
    "\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()  # 清空优化器梯度\n",
    "        loss.backward()  # 通过损失函数计算梯度\n",
    "        optimizer.step()  # 使用优化器对网络参数进行优化\n",
    "\n",
    "        # 打印损失函数 （每 16 组数据）\n",
    "        tqdm_process.set_description(\n",
    "            'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "# 定义测试流程\n",
    "@torch.no_grad()  # 表示在这个函数中，禁用 autograd 的梯度跟踪\n",
    "def eval_process():\n",
    "    # 使网络进入测试模式\n",
    "    net.eval()\n",
    "\n",
    "    # 记录损失（无梯度）\n",
    "    loss, correct = 0, 0\n",
    "\n",
    "    # 加载数据\n",
    "    tqdm_process = tqdm(eval_loader)  # 进度条\n",
    "    for data, target in tqdm_process:\n",
    "        # 若 CUDA 计算单元可用，将数据转移到 CUDA 中\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()  # 此处使用 .cuda()，对于高级用户 PyTorch 提供了更具体的操作 .to()\n",
    "\n",
    "        # 在一些古早的教程中，可能含有如下两行，是 PyTorch 0.4 的变量问题，目前已不再需要\n",
    "        # data, target = Variable(data, volatile=True), Variable(target)\n",
    "\n",
    "        # 前向传播 [size: 1024(batch_size), 1, 32, 32] -> net -> [size: 1024(batch_size), 10]\n",
    "        pred = net(data)\n",
    "\n",
    "        # 计算损失函数，这里选择 nll 损失\n",
    "        loss += nll_loss(pred, target, size_average=True)  # 对损失函数进行求和\n",
    "\n",
    "        # 统计正确个数\n",
    "        pred = pred.max(1, keepdim=True)[1]  # 获取具有最大可能性的那个数字\n",
    "        correct += pred.eq(target.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    # 展示损失与准确率\n",
    "    loss /= len(eval_loader.dataset)\n",
    "    print(\n",
    "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            loss, correct, len(eval_loader.dataset),\n",
    "            100. * correct / len(eval_loader.dataset)\n",
    "        )\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "至此，我们已经完成了一个简单深度学习的全部必要函数。接下来，我们将开始我们的训练。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(1, 10 + 1):\n",
    "    train_process(epoch)\n",
    "    eval_process()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}